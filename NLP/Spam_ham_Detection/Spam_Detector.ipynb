{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM-HAM Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the given dataset\n",
    "\n",
    "spam = pd.read_csv(\"SMSSpamCollection.txt\", sep = '\\t', names = ['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(spam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the read dataset in to a list of tuples, each tuple(row) contianing the message and it's label\n",
    "data_set  = []\n",
    "\n",
    "for index,row in spam.iterrows():\n",
    "    data_set.append((row['message'], row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'ham'), ('Ok lar... Joking wif u oni...', 'ham'), (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'spam'), ('U dun say so early hor... U c already then say...', 'ham'), (\"Nah I don't think he goes to usf, he lives around here though\", 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - Performing the preprocessing steps on all messages\n",
    "\n",
    "messages_set = []\n",
    "\n",
    "for (message, label) in data_set:\n",
    "    words_filtered = [e.lower() for e in preprocess(message, stem = False).split() if len(e)>3]\n",
    "    messages_set.append((words_filtered, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['jurong', 'point', 'crazy..', 'available', 'bugis', 'great', 'world', 'buffet', 'cine', 'amore'], 'ham'), (['joke'], 'ham'), (['free', 'entry', 'wkly', 'comp', 'final', 'tkts', '21st', '2005.', 'text', '87121', 'receive', 'entry', 'question', 'rate', 'apply', '08452810075over18'], 'spam'), (['early', 'already'], 'ham'), (['think', 'live', 'around', 'though'], 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(messages_set[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - creating a single list of all words in the entire dataset for feature list creation\n",
    "\n",
    "def get_words_in_messages(messages):\n",
    "    all_words = []\n",
    "    for (message, label) in messages:\n",
    "        all_words.extend(message)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - creating a final feature list using an intuitive FreqDist, to eliminate all the duplicate words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    \n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7483\n"
     ]
    }
   ],
   "source": [
    "## - creating the word features for the entire dataset\n",
    "\n",
    "word_features = get_word_features(get_words_in_messages(messages_set))\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to create a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - creating slicing index at 80% threshold\n",
    "\n",
    "sliceIndex = int((len(messages_set)*0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - shuffle the pack to create a random and unbiased split of the dataset\n",
    "\n",
    "random.shuffle(messages_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, test_messages = messages_set[:sliceIndex], messages_set[sliceIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4457"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to create feature maps for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a LazyMap of feature presence for each of the 8K+ features with respect to each of the SMS messages\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## - creating the feature map of train and test data\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, train_messages)\n",
    "testing_set = nltk.classify.apply_features(extract_features, test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  4457\n",
      "Test set size :  1115\n"
     ]
    }
   ],
   "source": [
    "print('Training set size : ', len(training_set))\n",
    "print('Test set size : ', len(testing_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the classifier with NaiveBayes algorithm\n",
    "\n",
    "spamclassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9914740857078752\n"
     ]
    }
   ],
   "source": [
    "## - Analyzing the accuracy of the train set\n",
    "\n",
    "print(nltk.classify.accuracy(spamclassifier, training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811659192825112\n"
     ]
    }
   ],
   "source": [
    "## Analyzing the accuracy of the test set\n",
    "\n",
    "print(nltk.classify.accuracy(spamclassifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(award) = True             spam : ham    =    207.7 : 1.0\n",
      "      contains(landline) = True             spam : ham    =    137.7 : 1.0\n",
      "          contains(code) = True             spam : ham    =    102.7 : 1.0\n",
      "        contains(camera) = True             spam : ham    =     98.4 : 1.0\n",
      "        contains(urgent) = True             spam : ham    =     94.6 : 1.0\n",
      "       contains(service) = True             spam : ham    =     92.5 : 1.0\n",
      "         contains(await) = True             spam : ham    =     89.6 : 1.0\n",
      "          contains(rate) = True             spam : ham    =     85.2 : 1.0\n",
      "         contains(video) = True             spam : ham    =     85.2 : 1.0\n",
      "         contains(nokia) = True             spam : ham    =     68.4 : 1.0\n",
      "       contains(private) = True             spam : ham    =     67.8 : 1.0\n",
      "          contains(draw) = True             spam : ham    =     64.6 : 1.0\n",
      "       contains(voucher) = True             spam : ham    =     63.4 : 1.0\n",
      "contains(congratulations) = True             spam : ham    =     63.4 : 1.0\n",
      "      contains(delivery) = True             spam : ham    =     63.4 : 1.0\n",
      "     contains(statement) = True             spam : ham    =     59.0 : 1.0\n",
      "        contains(mobile) = True             spam : ham    =     52.7 : 1.0\n",
      "          contains(quiz) = True             spam : ham    =     50.3 : 1.0\n",
      "          contains(land) = True             spam : ham    =     48.5 : 1.0\n",
      "        contains(latest) = True             spam : ham    =     47.8 : 1.0\n",
      "        contains(orange) = True             spam : ham    =     45.9 : 1.0\n",
      "       contains(auction) = True             spam : ham    =     41.5 : 1.0\n",
      "        contains(todays) = True             spam : ham    =     40.7 : 1.0\n",
      "        contains(colour) = True             spam : ham    =     40.7 : 1.0\n",
      "         contains(apply) = True             spam : ham    =     40.3 : 1.0\n",
      "          contains(line) = True             spam : ham    =     39.9 : 1.0\n",
      "       contains(attempt) = True             spam : ham    =     38.0 : 1.0\n",
      "         contains(final) = True             spam : ham    =     38.0 : 1.0\n",
      "          contains(info) = True             spam : ham    =     38.0 : 1.0\n",
      "          contains(cash) = True             spam : ham    =     34.0 : 1.0\n",
      "      contains(sunshine) = True             spam : ham    =     32.8 : 1.0\n",
      "       contains(england) = True             spam : ham    =     32.8 : 1.0\n",
      "         contains(music) = True             spam : ham    =     30.9 : 1.0\n",
      "         contains(offer) = True             spam : ham    =     29.8 : 1.0\n",
      "      contains(customer) = True             spam : ham    =     29.3 : 1.0\n",
      "        contains(flight) = True             spam : ham    =     28.4 : 1.0\n",
      "          contains(sony) = True             spam : ham    =     28.4 : 1.0\n",
      "          contains(user) = True             spam : ham    =     28.4 : 1.0\n",
      "          contains(comp) = True             spam : ham    =     28.4 : 1.0\n",
      "        contains(txting) = True             spam : ham    =     28.4 : 1.0\n",
      "        contains(arrive) = True             spam : ham    =     28.4 : 1.0\n",
      "        contains(select) = True             spam : ham    =     26.8 : 1.0\n",
      "       contains(network) = True             spam : ham    =     26.8 : 1.0\n",
      "        contains(player) = True             spam : ham    =     24.9 : 1.0\n",
      "      contains(discount) = True             spam : ham    =     24.9 : 1.0\n",
      "        contains(access) = True             spam : ham    =     24.0 : 1.0\n",
      "        contains(summer) = True             spam : ham    =     24.0 : 1.0\n",
      "        contains(reveal) = True             spam : ham    =     24.0 : 1.0\n",
      "          contains(goto) = True             spam : ham    =     24.0 : 1.0\n",
      "        contains(member) = True             spam : ham    =     24.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Priting the most informative features in the classifier\n",
    "print(spamclassifier.show_most_informative_features(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CONGRATULATIONS!! As a valued account holder you have been selected to receive a Â£900 prize reward! Valid 12 hours only.'\n",
      "Classification result :  spam\n"
     ]
    }
   ],
   "source": [
    "input_msg = input()\n",
    "print('Classification result : ', spamclassifier.classify(extract_features(input_msg.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
